"""
ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- Router: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
- Multi-Query Generator: ë‹¤ì¤‘ ê´€ì  ì¿¼ë¦¬ ìƒì„±
- Ensemble Retriever: Dense + BM25
- RRF (Reciprocal Rank Fusion): ê²€ìƒ‰ ê²°ê³¼ í†µí•©
- Memory Store: ëŒ€í™” ë§¥ë½ ê´€ë¦¬
"""

import os
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
import json
import warnings

# TensorFlow ë¡œê·¸ ì–µì œ (dotenv ë¡œë“œ ì „ì— ì„¤ì •)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document

# BM25, Ensemble Retriever
try:
    # LangChain deprecation ê²½ê³  ë¬´ì‹œ
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        from langchain_classic.retrievers import BM25Retriever, EnsembleRetriever
    RETRIEVERS_AVAILABLE = True
except ImportError:
    RETRIEVERS_AVAILABLE = False
    BM25Retriever = None
    EnsembleRetriever = None

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Retrievers ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
if not RETRIEVERS_AVAILABLE:
    print("âš ï¸ BM25 Retrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("âš ï¸ ì„¤ì¹˜: pip install langchain-community")



# ============================================================================
# 1. Router: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
# ============================================================================

class QueryRouter:
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.router_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

            ì‘ì—…:
            1. ì§ˆë¬¸ì´ ì˜ë¯¸ ìˆëŠ”ì§€ ê²€ì¦ (ì¸ì‚¬ë§, ìš•ì„¤, ë¬´ì˜ë¯¸í•œ ì…ë ¥ ì œì™¸)
            2. ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì •ì±…ê²€ìƒ‰, ì¶”ì²œ, ì¼ë°˜ì§ˆë¬¸ ë“±)
            3. LLMì´ ì²˜ë¦¬í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì •ì œ

            ì‘ë‹µ í˜•ì‹ (JSON):
            {{
                "is_valid": true/false,
                "category": "ì •ì±…ê²€ìƒ‰|ì •ì±…ì¶”ì²œ|ì¼ë°˜ì§ˆë¬¸|ê¸°íƒ€",
                "refined_query": "ì •ì œëœ ì§ˆë¬¸",
                "reason": "íŒë‹¨ ì´ìœ "
            }}"""),
                        ("user", "{query}")
        ])
    
    def route(self, query: str) -> Dict:
        """ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œ"""
        try:
            response = self.router_prompt | self.llm | StrOutputParser()
            result_str = response.invoke({"query": query})
            
            # JSON íŒŒì‹±
            result = json.loads(result_str)
            print(f"ğŸ”€ Router: {result['category']} | Valid: {result['is_valid']}")
            
            return result
        except Exception as e:
            print(f"âŒ Router Error: {e}")
            return {
                "is_valid": True,
                "category": "ì¼ë°˜ì§ˆë¬¸",
                "refined_query": query,
                "reason": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ì›ë³¸ ì‚¬ìš©"
            }


# ============================================================================
# 2. Multi-Query Generator: ë‹¤ì¤‘ ê´€ì  ì¿¼ë¦¬ ìƒì„±
# ============================================================================

class MultiQueryGenerator:
    """í•˜ë‚˜ì˜ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ í™•ì¥"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        
        self.multi_query_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë‹¤ì–‘í•œ ê´€ì ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ì„¸ìš”:
1. í‚¤ì›Œë“œ ì¤‘ì‹¬ ì¿¼ë¦¬
2. ì˜ë¯¸ ì¤‘ì‹¬ ì¿¼ë¦¬
3. ë§¥ë½ ì¤‘ì‹¬ ì¿¼ë¦¬

{region_instruction}

            ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ ì—†ì´ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”."""),
                        ("user", "{query}")
        ])
    
    def generate(self, query: str) -> List[str]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±"""
        try:
            response = self.multi_query_prompt | self.llm | StrOutputParser()
            result = response.invoke({"query": query})
            
            # ì¿¼ë¦¬ ë¶„ë¦¬ (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
            queries = [q.strip() for q in result.split('\n') if q.strip()]
            # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
            all_queries = [query] + queries
            
            print(f"ğŸ” Multi-Query ìƒì„±: {len(all_queries)}ê°œ")
            for i, q in enumerate(all_queries, 1):
                print(f"  {i}. {q}")
            
            return all_queries
        except Exception as e:
            print(f"âŒ Multi-Query Error: {e}")
            return [query]


# ============================================================================
# 3. Ensemble Retriever: ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ
# ============================================================================

class EnsembleRetriever:
    """Dense, BM25 ê²€ìƒ‰ì„ ê²°í•©í•œ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„"""
    
    def __init__(
        self, 
        documents: List[any],
        vectorstore: Chroma,
        bm25_k: int = 5,
        vector_k: int = 10,
        bm25_weight: float = 0.4,
        vector_weight: float = 0.6
    ):
        self.documents = documents
        self.vectorstore = vectorstore
        
        # íŒŒë¼ë¯¸í„° ì €ì¥
        self.bm25_k = bm25_k
        self.vector_k = vector_k
        self.bm25_weight = bm25_weight
        self.vector_weight = vector_weight
        
        # ê° ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
        self._build_bm25()
        self._build_vector()
    
    def _build_bm25(self):
        """BM25 Retriever ìƒì„±"""
        if not RETRIEVERS_AVAILABLE or BM25Retriever is None:
            print("âš ï¸ BM25Retrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.bm25_retriever = None
            return
        
        if not self.documents:
            print("âš ï¸ BM25: ë¬¸ì„œê°€ ì—†ì–´ ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            self.bm25_retriever = None
            return
        
        try:
            # BM25Retriever ì´ˆê¸°í™” (from_documents ì‚¬ìš©)
            self.bm25_retriever = BM25Retriever.from_documents(
                documents=self.documents,
                k=self.bm25_k
            )
            print(f"âœ… BM25 Retriever ì´ˆê¸°í™” ì™„ë£Œ (k={self.bm25_k})")
        except TypeError as e:
            # from_documentsê°€ ì‹¤íŒ¨í•˜ë©´ ì§ì ‘ ì´ˆê¸°í™” ì‹œë„
            try:
                self.bm25_retriever = BM25Retriever(docs=self.documents)
                self.bm25_retriever.k = self.bm25_k
                print(f"âœ… BM25 Retriever ì´ˆê¸°í™” ì™„ë£Œ (ëŒ€ì²´ ë°©ì‹, k={self.bm25_k})")
            except Exception as e2:
                print(f"âŒ BM25 Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                self.bm25_retriever = None
        except Exception as e:
            print(f"âŒ BM25 Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.bm25_retriever = None
    
    def _build_vector(self):
        """Vector Retriever ìƒì„±"""
        try:
            # VectorStore ìƒíƒœ í™•ì¸
            test_search = self.vectorstore.similarity_search("í…ŒìŠ¤íŠ¸", k=1)
            print(f"ğŸ§ª VectorStore í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: {len(test_search)}ê°œ ë¬¸ì„œ")
            
            self.vector_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.vector_k}
            )
            print(f"âœ… Vector Retriever ì´ˆê¸°í™” ì™„ë£Œ (k={self.vector_k})")
        except Exception as e:
            print(f"âŒ Vector Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.vector_retriever = None
    
    def dense_search(self, query: str) -> List[Tuple[any, float]]:
        """Dense ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)"""
        try:
            if self.vector_retriever:
                docs = self.vector_retriever.invoke(query)
                # ìŠ¤ì½”ì–´ì™€ í•¨ê»˜ ë°˜í™˜ (ìŠ¤ì½”ì–´ëŠ” 1.0ìœ¼ë¡œ ê°€ì •)
                results = [(doc, 1.0) for doc in docs]
                print(f"  ğŸ“Š Dense: {len(results)}ê°œ ë¬¸ì„œ")
                return results
            return []
        except Exception as e:
            print(f"âŒ Dense Search Error: {e}")
            return []
    
    def bm25_search(self, query: str) -> List[Tuple[any, float]]:
        """BM25 ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        try:
            if self.bm25_retriever:
                docs = self.bm25_retriever.invoke(query)
                results = [(doc, 1.0) for doc in docs]
                print(f"  ğŸ“Š BM25: {len(results)}ê°œ ë¬¸ì„œ")
                return results
            return []
        except Exception as e:
            print(f"âŒ BM25 Search Error: {e}")
            return []
    
    def retrieve(self, queries: List[str]) -> Dict[str, List[Tuple[any, float]]]:
        """ëª¨ë“  ê²€ìƒ‰ ì „ëµ ì‹¤í–‰"""
        all_results = {
            'dense': [],
            'bm25': []
        }
        
        for query in queries:
            print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
            all_results['dense'].extend(self.dense_search(query))
            all_results['bm25'].extend(self.bm25_search(query))
        
        return all_results
    
    def get_ensemble(self, query: str) -> List[any]:
        """Ensemble ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ì ìš©)"""
        if not RETRIEVERS_AVAILABLE or EnsembleRetriever is None:
            print("âš ï¸ EnsembleRetrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Vector ê²€ìƒ‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self.dense_search(query)
        
        try:
            retrievers = []
            weights = []
            
            if self.bm25_retriever:
                retrievers.append(self.bm25_retriever)
                weights.append(self.bm25_weight)
            
            if self.vector_retriever:
                retrievers.append(self.vector_retriever)
                weights.append(self.vector_weight)
            
            if not retrievers:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ retrieverê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total_weight = sum(weights)
            weights = [w / total_weight for w in weights]
            
            # LangChainì˜ EnsembleRetriever ì‚¬ìš©
            ensemble = EnsembleRetriever(
                retrievers=retrievers,
                weights=weights
            )
            
            docs = ensemble.invoke(query)
            print(f"ğŸ”— Ensemble: {len(docs)}ê°œ ë¬¸ì„œ")
            return docs
            
        except Exception as e:
            print(f"âŒ Ensemble Search Error: {e}")
            return []


# ============================================================================
# 4. RRF (Reciprocal Rank Fusion): ê²€ìƒ‰ ê²°ê³¼ í†µí•©
# ============================================================================

class ReciprocalRankFusion:
    """ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë­í‚¹ ê¸°ë°˜ìœ¼ë¡œ í†µí•©"""
    
    def __init__(self, k: int = 60):
        self.k = k  # RRF ìƒìˆ˜
    
    def fuse(self, results_dict: Dict[str, List[Tuple[any, float]]], top_k: int = 10) -> List[any]:
        """RRFë¡œ ê²°ê³¼ í†µí•©"""
        doc_scores = {}
        
        for method, results in results_dict.items():
            for rank, (doc, score) in enumerate(results, 1):
                doc_id = doc.metadata.get('policy_id', id(doc))
                
                # RRF ì ìˆ˜ ê³„ì‚°: 1 / (k + rank)
                rrf_score = 1.0 / (self.k + rank)
                
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        final_docs = [item[1]['doc'] for item in sorted_docs[:top_k]]
        
        print(f"ğŸ”— RRF: {len(doc_scores)}ê°œ ë¬¸ì„œ â†’ {len(final_docs)}ê°œ ì„ íƒ")
        return final_docs


# ============================================================================
# 5. Memory Store: ëŒ€í™” ë§¥ë½ ê´€ë¦¬
# ============================================================================

@dataclass
class ConversationMemory:
    """ëŒ€í™” ê¸°ë¡ ê´€ë¦¬"""
    messages: List[Dict] = field(default_factory=list)
    max_history: int = 10
    
    def add_message(self, role: str, content: str):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # ìµœëŒ€ ê¸°ë¡ ìˆ˜ ì œí•œ
        if len(self.messages) > self.max_history * 2:
            self.messages = self.messages[-self.max_history * 2:]
    
    def get_context(self) -> str:
        """ëŒ€í™” ë§¥ë½ ë¬¸ìì—´ ìƒì„±"""
        if not self.messages:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        
        context_parts = []
        for msg in self.messages[-6:]:  # ìµœê·¼ 3í„´
            role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "AI"
            context_parts.append(f"{role}: {msg['content']}")
        
        return "\n".join(context_parts)
    
    def clear(self):
        """ê¸°ë¡ ì´ˆê¸°í™”"""
        self.messages.clear()


# ============================================================================
# 7. Advanced RAG Pipeline: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
# ============================================================================

class AdvancedRAGPipeline:
    """ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        documents: List[any],
        vectorstore: Chroma,
        llm: ChatOpenAI,
        enable_router: bool = True,
        enable_multi_query: bool = True,
        enable_ensemble: bool = True,
        enable_rrf: bool = True,
        enable_memory: bool = True,
        bm25_k: int = 5,
        vector_k: int = 10,
        bm25_weight: float = 0.4,
        vector_weight: float = 0.6
    ):
        self.documents = documents
        self.vectorstore = vectorstore
        self.llm = llm
        
        # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = QueryRouter(llm) if enable_router else None
        self.multi_query = MultiQueryGenerator(llm) if enable_multi_query else None
        self.ensemble = EnsembleRetriever(
            documents=documents,
            vectorstore=vectorstore,
            bm25_k=bm25_k,
            vector_k=vector_k,
            bm25_weight=bm25_weight,
            vector_weight=vector_weight
        ) if enable_ensemble else None
        self.rrf = ReciprocalRankFusion() if enable_rrf else None
        self.memory = ConversationMemory() if enable_memory else None
        
        # ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.answer_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì²­ë…„ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

            ê²€ìƒ‰ëœ ì •ì±… ì •ë³´ì™€ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
1. ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
2. ì •ì±…ëª…, ì‹ ì²­ ê¸°ê°„, ì§€ì› ë‚´ìš© ë“± êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
3. ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
4. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§í•˜ê²Œ ë§í•˜ê¸°
5. **ì œê³µëœ ëª¨ë“  ì •ì±…ì„ ê°€ëŠ¥í•œ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”** (ìµœì†Œ 3ê°œ ì´ìƒ)"""),
            ("user", """[ëŒ€í™” ë§¥ë½]
{context}

[ì‚¬ìš©ì í”„ë¡œí•„]
{profile}

            [ê²€ìƒ‰ëœ ì •ì±… ì •ë³´]
            {documents}

[í˜„ì¬ ì§ˆë¬¸]
{query}""")
        ])
    
    def query(self, user_query: str) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {user_query}")
        print(f"{'='*60}")
        
        # 1. Router: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
        if self.router:
            route_result = self.router.route(user_query)
            if not route_result['is_valid']:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                    "documents": [],
                    "metadata": route_result
                }
            query = route_result['refined_query']
        else:
            query = user_query
        
        # 2. Multi-Query: ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
        if self.multi_query:
            queries = self.multi_query.generate(query)
        else:
            queries = [query]
        
        # 3. Ensemble Retriever: ë‹¤ì¤‘ ê²€ìƒ‰
        if self.ensemble:
            search_results = self.ensemble.retrieve(queries)
        else:
            search_results = {'dense': self.vectorstore.similarity_search_with_score(query, k=5)}
        
        # 4. RRF: ê²€ìƒ‰ ê²°ê³¼ í†µí•© (top_k ì¦ê°€)
        if self.rrf:
            docs = self.rrf.fuse(search_results, top_k=20)
        else:
            docs = [doc for doc, score in search_results['dense']]
        
        # 5. Memory: ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸°
        if self.memory:
            context = self.memory.get_context()
        else:
            context = "ì´ì „ ëŒ€í™” ì—†ìŒ"
        
        # 6. LLM: ìµœì¢… ë‹µë³€ ìƒì„±
        docs_text = "\n\n".join([
            f"[ì •ì±… {i+1}] {doc.metadata.get('policy_name', 'ì œëª© ì—†ìŒ')}\n{doc.page_content[:500]}"
            for i, doc in enumerate(docs[:10])
        ])
        
        try:
            response = self.answer_prompt | self.llm | StrOutputParser()
            answer = response.invoke({
                "context": context,
                "documents": docs_text,
                "query": user_query
            })
            
            # 7. ìš”ì•½ ìƒì„± (Chain of Thought)
            summary_response = self.summary_prompt | self.llm | StrOutputParser()
            summary = summary_response.invoke({"answer": answer})
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            if self.memory:
                self.memory.add_message("user", user_query)
                self.memory.add_message("assistant", answer)
            
            print(f"\nâœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            print(f"ğŸ“Œ ìš”ì•½ ìƒì„± ì™„ë£Œ")
            print(f"{'='*60}\n")
            
            return {
                "answer": answer,
                "summary": summary,
                "documents": docs,
                "metadata": {
                    "queries": queries,
                    "num_docs_retrieved": len(docs),
                    "has_context": bool(self.memory and self.memory.messages)
                }
            }
            
        except Exception as e:
            print(f"âŒ Answer Generation Error: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "documents": [],
                "metadata": {"error": str(e)}
            }
    
    def clear_memory(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        if self.memory:
            self.memory.clear()
            print("ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================================
# 8. ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

def main():
    """ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    
    # LLM ë° ì„ë² ë”© ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.7,
        api_key=api_key
    )
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=api_key
    )
    
    # VectorDB ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    vectordb_path = os.path.join(project_root, "data", "vectordb")
    
    print(f"ğŸ“‚ VectorDB ê²½ë¡œ: {vectordb_path}")
    print(f"ğŸ“‚ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(vectordb_path)}")
    
    if not os.path.exists(vectordb_path):
        print("âŒ VectorDB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. build_vectordb.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    vectorstore = Chroma(
        collection_name="youth_policies",
        embedding_function=embeddings,
        persist_directory=vectordb_path
    )
    
    # ë¬¸ì„œ ë¡œë“œ (BM25ë¥¼ ìœ„í•´ í•„ìš”)
    # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    all_docs = vectorstore.get()
    print(f"ğŸ“Š ChromaDB ë¡œë“œ ê²°ê³¼: {len(all_docs.get('documents', []))}ê°œ ë¬¸ì„œ")
    
    if not all_docs or not all_docs.get('documents'):
        print("âŒ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. build_vectordb.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    documents = []
    if all_docs and 'documents' in all_docs:
        from langchain_core.documents import Document
        for i, doc_text in enumerate(all_docs['documents']):
            if doc_text and doc_text.strip():  # ë¹ˆ ë¬¸ì„œ ì œì™¸
                metadata = all_docs['metadatas'][i] if 'metadatas' in all_docs else {}
                documents.append(Document(page_content=doc_text, metadata=metadata))
    
    print(f"ğŸ“š ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    # ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
    rag = AdvancedRAGPipeline(
        documents=documents,
        vectorstore=vectorstore,
        llm=llm,
        enable_router=True,
        enable_multi_query=True,
        enable_ensemble=True,
        enable_rrf=True,
        enable_memory=True,
        bm25_k=5,
        vector_k=10,
        bm25_weight=0.4,
        vector_weight=0.6
    )
    
    # í…ŒìŠ¤íŠ¸ ì§ˆì˜
    queries = [
        "ëŒ€êµ¬ ì›”ì„¸ ì§€ì›",
    ]
    
    for query in queries:
        result = rag.query(query)
        print(f"\nì§ˆë¬¸: {query}")
        print(f"\nğŸ“„ ì „ì²´ ë‹µë³€:\n{result['answer']}")
        print(f"\nğŸ“Œ ìš”ì•½:\n{result['summary']}")
        print(f"\në¬¸ì„œ ìˆ˜: {result['metadata'].get('num_docs_retrieved', 0)}")
        print("-" * 60)


if __name__ == "__main__":
    main()
