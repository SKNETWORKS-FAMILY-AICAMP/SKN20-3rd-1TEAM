# ğŸ“¦ ê°œë°œëœ ì†Œí”„íŠ¸ì›¨ì–´: RAG ê¸°ë°˜ LLMê³¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ êµ¬í˜„ ì½”ë“œ

## ëª©ì°¨
1. [ì†Œí”„íŠ¸ì›¨ì–´ ê°œìš”](#1-ì†Œí”„íŠ¸ì›¨ì–´-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [í•µì‹¬ êµ¬í˜„ ëª¨ë“ˆ](#3-í•µì‹¬-êµ¬í˜„-ëª¨ë“ˆ)
4. [RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„](#4-rag-íŒŒì´í”„ë¼ì¸-êµ¬í˜„)
5. [ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™](#5-ë²¡í„°-ë°ì´í„°ë² ì´ìŠ¤-ì—°ë™)
6. [ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„](#6-ì›¹-ì¸í„°í˜ì´ìŠ¤-êµ¬í˜„)
7. [ì£¼ìš” ê¸°ëŠ¥ ì½”ë“œ](#7-ì£¼ìš”-ê¸°ëŠ¥-ì½”ë“œ)
8. [API ë° ì™¸ë¶€ ì—°ë™](#8-api-ë°-ì™¸ë¶€-ì—°ë™)

---

## 1. ì†Œí”„íŠ¸ì›¨ì–´ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ëª…
**ì²­ë…„ ì •ì±… Q&A ì±—ë´‡ (ì˜¨í†µì²­ë…„ RAG ì‹œìŠ¤í…œ)**

### 1.2 ê°œë°œ ëª©ì 
- í™˜ê°(Hallucination)ì„ ìµœì†Œí™”í•˜ê³  ì •í™•í•œ ì²­ë…„ ì •ì±… ì •ë³´ ì œê³µ
- RAG(Retrieval-Augmented Generation) ê¸°ìˆ ì„ í™œìš©í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ ìƒì„±
- 3,550ê°œ ì²­ë…„ ì •ì±…ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì—¬ ì‹¤ì‹œê°„ ê²€ìƒ‰ ë° ì‘ë‹µ

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

| ë¶„ë¥˜ | ê¸°ìˆ  | ë²„ì „ |
|------|------|------|
| **ì–¸ì–´** | Python | 3.11+ |
| **LLM** | OpenAI GPT-4o-mini | - |
| **ì„ë² ë”©** | OpenAI text-embedding-3-small | 1,536ì°¨ì› |
| **ë²¡í„° DB** | ChromaDB | 0.4.0+ |
| **í”„ë ˆì„ì›Œí¬** | LangChain | 0.1.0+ |
| **ì›¹ UI** | Streamlit | 1.28.0+ |
| **ê²€ìƒ‰** | BM25 + Vector Search | - |

### 1.4 í”„ë¡œì íŠ¸ êµ¬ì¡°
```
SKN20-3rd-1TEAM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ advanced_rag_pipeline.py    # ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ (í•µì‹¬)
â”‚   â””â”€â”€ streamlit_app.py            # Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ fetch_api_data.py           # ë°ì´í„° ìˆ˜ì§‘
â”‚   â”œâ”€â”€ build_vectordb.py           # ë²¡í„° DB êµ¬ì¶•
â”‚   â””â”€â”€ youth_policy_rag.py         # ê¸°ë³¸ RAG ì‹œìŠ¤í…œ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # ì›ë³¸ ë°ì´í„° (11.71 MB)
â”‚   â”œâ”€â”€ processed/                  # ì „ì²˜ë¦¬ ë°ì´í„° (11.29 MB)
â”‚   â””â”€â”€ vectordb/                   # ë²¡í„° DB (87 MB)
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â”œâ”€â”€ requirements.txt                # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â””â”€â”€ .env                            # í™˜ê²½ ë³€ìˆ˜ (API Keys)
```

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°
```
[ì‚¬ìš©ì] 
    â†“ ì§ˆë¬¸ ì…ë ¥
[Streamlit UI]
    â†“ 
[Advanced RAG Pipeline] â† 7ë‹¨ê³„ ì²˜ë¦¬
    â†“ ê²€ìƒ‰ ì¿¼ë¦¬
[ChromaDB Vector Store] (3,550ê°œ ì •ì±…)
    â†“ ì •ì±… ë¬¸ì„œ ë°˜í™˜
[OpenAI GPT-4o-mini] â† 4íšŒ LLM í˜¸ì¶œ
    â†“ ë‹µë³€ + ìš”ì•½
[Streamlit UI] â†’ ì‚¬ìš©ìì—ê²Œ í‘œì‹œ
```

### 2.2 RAG íŒŒì´í”„ë¼ì¸ (7ë‹¨ê³„)
1. **Query Router**: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ (LLM í˜¸ì¶œ 1íšŒ)
2. **Multi-Query Generator**: 3ê°œ ì¿¼ë¦¬ ìƒì„± (LLM í˜¸ì¶œ 1íšŒ)
3. **Ensemble Retriever**: BM25 (40%) + Vector (60%) ë³‘í•© ê²€ìƒ‰
4. **RRF (Reciprocal Rank Fusion)**: Top 20ê°œ ë¬¸ì„œ í†µí•©
5. **Conversation Memory**: ìµœê·¼ 3í„´ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
6. **LLM Answer Generation**: ìƒìœ„ 10ê°œ ì •ì±… ê¸°ë°˜ ë‹µë³€ (LLM í˜¸ì¶œ 1íšŒ)
7. **Summary Generation**: Chain of Thought ìš”ì•½ (LLM í˜¸ì¶œ 1íšŒ)

### 2.3 ë°ì´í„° íë¦„
```mermaid
graph LR
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[Router]
    B --> C[MultiQuery]
    C --> D[BM25 40%]
    C --> E[Vector 60%]
    D --> F[RRF Fusion]
    E --> F
    F --> G[Memory]
    G --> H[LLM Answer]
    H --> I[Summary]
    I --> J[UI í‘œì‹œ]
```

---

## 3. í•µì‹¬ êµ¬í˜„ ëª¨ë“ˆ

### 3.1 íŒŒì¼ ê°œìš”

#### `src/advanced_rag_pipeline.py` (792 lines)
**ì—­í• **: ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ ë¡œì§

**í¬í•¨ í´ë˜ìŠ¤**:
- `QueryRouter`: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
- `MultiQueryGenerator`: ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
- `EnsembleRetriever`: BM25 + Vector ê²€ìƒ‰
- `ReciprocalRankFusion`: ê²€ìƒ‰ ê²°ê³¼ í†µí•©
- `ConversationMemory`: ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
- `AdvancedRAGPipeline`: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©

**ì£¼ìš” í•¨ìˆ˜**:
- `initialize_rag_pipeline()`: Streamlit ì—°ë™ìš© ì´ˆê¸°í™” í•¨ìˆ˜

#### `src/streamlit_app.py` (236 lines)
**ì—­í• **: ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

**ì£¼ìš” ê¸°ëŠ¥**:
- CSS ìŠ¤íƒ€ì¼ ì ìš©
- RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ (`@st.cache_resource`)
- ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
- ìš”ì•½ + ì „ì²´ ë‹µë³€ í‘œì‹œ
- ì •ì±… ì¹´ë“œ Expander

#### `notebooks/build_vectordb.py` (572 lines)
**ì—­í• **: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•

**ì£¼ìš” ê¸°ëŠ¥**:
- JSON ë°ì´í„° ë¡œë“œ
- OpenAI ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
- ChromaDB ì €ì¥
- ë©”íƒ€ë°ì´í„° ë§¤í•‘

---

## 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### 4.1 QueryRouter (ì§ˆë¬¸ ê²€ì¦)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 52-100)

**ëª©ì **: ì‚¬ìš©ì ì§ˆë¬¸ì˜ ìœ íš¨ì„± ê²€ì¦ ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜

**ì½”ë“œ**:
```python
class QueryRouter:
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.router_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

            ì‘ì—…:
            1. ì§ˆë¬¸ì´ ì˜ë¯¸ ìˆëŠ”ì§€ ê²€ì¦ (ì¸ì‚¬ë§, ìš•ì„¤, ë¬´ì˜ë¯¸í•œ ì…ë ¥ ì œì™¸)
            2. ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì •ì±…ê²€ìƒ‰, ì¶”ì²œ, ì¼ë°˜ì§ˆë¬¸ ë“±)
            3. LLMì´ ì²˜ë¦¬í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì •ì œ
            4. ì§€ì—­ì´ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ë‹¤ì‹œ ì…ë ¥í•˜ë„ë¡ ìœ ë„
            
            ì‘ë‹µ í˜•ì‹ (JSON):
            {{
                "is_valid": true/false,
                "category": "ì •ì±…ê²€ìƒ‰|ì •ì±…ì¶”ì²œ|ì¼ë°˜ì§ˆë¬¸|ê¸°íƒ€",
                "refined_query": "ì •ì œëœ ì§ˆë¬¸",
                "reason": "íŒë‹¨ ì´ìœ "
            }}"""),
            ("user", "{query}")
        ])
    
    def route(self, query: str) -> Dict:
        """ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œ"""
        try:
            response = self.router_prompt | self.llm | StrOutputParser()
            result_str = response.invoke({"query": query})
            
            # JSON íŒŒì‹±
            result = json.loads(result_str)
            print(f"ğŸ”€ Router: {result['category']} | Valid: {result['is_valid']}")
            
            return result
        except Exception as e:
            print(f"âŒ Router Error: {e}")
            return {
                "is_valid": True,
                "category": "ì¼ë°˜ì§ˆë¬¸",
                "refined_query": query,
                "reason": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ì›ë³¸ ì‚¬ìš©"
            }
```

**íŠ¹ì§•**:
- LLMì„ í™œìš©í•œ ì§€ëŠ¥í˜• ë¼ìš°íŒ…
- JSON í˜•ì‹ ì‘ë‹µìœ¼ë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼
- ì—ëŸ¬ í•¸ë“¤ë§ (íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©)

---

### 4.2 MultiQueryGenerator (ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 105-160)

**ëª©ì **: í•˜ë‚˜ì˜ ì§ˆë¬¸ì„ 3ê°œì˜ ë‹¤ì–‘í•œ ê´€ì  ì¿¼ë¦¬ë¡œ í™•ì¥

**ì½”ë“œ**:
```python
class MultiQueryGenerator:
    """í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê´€ì ìœ¼ë¡œ í™•ì¥"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.multi_query_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ 3ê°œì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ëª©í‘œ: ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ (ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ì¡°í•©)

ê·œì¹™:
1. ì›ë˜ ì˜ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í‘œí˜„ ë°©ì‹ì„ ë³€ê²½
2. ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´ë¥¼ í™œìš©
3. êµ¬ì²´ì  â†’ ì¶”ìƒì , ì¶”ìƒì  â†’ êµ¬ì²´ì  ë“± ë‹¤ì–‘í™”
4. ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ì”© êµ¬ë¶„ (ë²ˆí˜¸ ì—†ì´)

ì˜ˆì‹œ:
ì…ë ¥: "ì„œìš¸ì—ì„œ ì·¨ì—… ì¤€ë¹„ìƒì„ ìœ„í•œ ì§€ì› ì •ì±…ì€?"
ì¶œë ¥:
ì„œìš¸ì‹œ ì²­ë…„ êµ¬ì§ì ëŒ€ìƒ ì·¨ì—…ì§€ì› í”„ë¡œê·¸ë¨
ìˆ˜ë„ê¶Œ ë¯¸ì·¨ì—… ì²­ë…„ ëŒ€ìƒ ì¼ìë¦¬ ì •ì±…
ì„œìš¸ ê±°ì£¼ ì·¨ì¤€ìƒì„ ìœ„í•œ ê¸ˆìœµÂ·êµìœ¡ ì§€ì› ì‚¬ì—…
"""),
            ("user", "{query}")
        ])
    
    def generate(self, query: str) -> List[str]:
        """3ê°œì˜ ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±"""
        try:
            response = self.multi_query_prompt | self.llm | StrOutputParser()
            result = response.invoke({"query": query})
            
            # ìƒì„±ëœ ì¿¼ë¦¬ íŒŒì‹± (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
            queries = [q.strip() for q in result.split('\n') if q.strip()]
            
            # ìƒì„±ëœ ì¿¼ë¦¬ë§Œ ë°˜í™˜ (ì›ë³¸ ì œì™¸)
            print(f"ğŸ” Multi-Query Generated: {len(queries)}ê°œ")
            for i, q in enumerate(queries, 1):
                print(f"  {i}. {q}")
            
            return queries
            
        except Exception as e:
            print(f"âŒ Multi-Query Error: {e}")
            return [query]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ë§Œ ë°˜í™˜
```

**íŠ¹ì§•**:
- ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ (ë‹¨ì¼ ì¿¼ë¦¬ â†’ 3ê°œ ì¿¼ë¦¬)
- ë™ì˜ì–´ ë° ê´€ë ¨ ìš©ì–´ ìë™ ìƒì„±
- ì›ë³¸ ì¿¼ë¦¬ ì œì™¸ (ìƒì„±ëœ 3ê°œë§Œ ì‚¬ìš©)

---

### 4.3 EnsembleRetriever (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 165-310)

**ëª©ì **: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ + ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°í•©

**ì½”ë“œ**:
```python
class EnsembleRetriever:
    """BM25 (í‚¤ì›Œë“œ) + Vector (ì˜ë¯¸) ê²€ìƒ‰ ê²°í•©"""
    
    def __init__(
        self,
        documents: List[Document],
        vectorstore: Chroma,
        bm25_k: int = 20,
        vector_k: int = 20,
        bm25_weight: float = 0.4,
        vector_weight: float = 0.6
    ):
        self.documents = documents
        self.vectorstore = vectorstore
        self.bm25_k = bm25_k
        self.vector_k = vector_k
        self.bm25_weight = bm25_weight
        self.vector_weight = vector_weight
        
        # BM25 Retriever ì´ˆê¸°í™”
        self._initialize_bm25_retriever()
    
    def _initialize_bm25_retriever(self):
        """BM25 Retriever ì´ˆê¸°í™” (2ë‹¨ê³„ í´ë°±)"""
        if not RETRIEVERS_AVAILABLE:
            self.bm25_retriever = None
            return
        
        if not self.documents:
            print("âš ï¸ ë¬¸ì„œê°€ ì—†ì–´ BM25ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.bm25_retriever = None
            return
        
        try:
            # ë°©ë²• 1: from_documents (ê¶Œì¥)
            self.bm25_retriever = BM25Retriever.from_documents(
                documents=self.documents,
                k=self.bm25_k
            )
            print(f"âœ… BM25 Retriever ì´ˆê¸°í™” ì™„ë£Œ (ë¬¸ì„œ {len(self.documents)}ê°œ)")
            
        except TypeError:
            # ë°©ë²• 2: ì§ì ‘ ì´ˆê¸°í™” (í´ë°±)
            try:
                self.bm25_retriever = BM25Retriever(
                    docs=self.documents,
                    k=self.bm25_k
                )
                print(f"âœ… BM25 Retriever ì´ˆê¸°í™” ì™„ë£Œ (í´ë°± ë°©ì‹)")
            except Exception as e:
                print(f"âŒ BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.bm25_retriever = None
    
    def retrieve(self, query: str) -> List[Document]:
        """BM25 + Vector ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        bm25_docs = []
        vector_docs = []
        
        # 1. BM25 ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­)
        if self.bm25_retriever:
            try:
                bm25_docs = self.bm25_retriever.get_relevant_documents(query)
                print(f"  BM25: {len(bm25_docs)}ê°œ")
            except Exception as e:
                print(f"âŒ BM25 ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # 2. Vector ê²€ìƒ‰ (ì˜ë¯¸ ìœ ì‚¬ë„)
        try:
            vector_docs = self.vectorstore.similarity_search(query, k=self.vector_k)
            print(f"  Vector: {len(vector_docs)}ê°œ")
        except Exception as e:
            print(f"âŒ Vector ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # 3. ê°€ì¤‘ì¹˜ ê¸°ë°˜ í†µí•© (Ensemble)
        ensemble_docs = self._weighted_merge(bm25_docs, vector_docs)
        
        print(f"âœ… Ensemble ê²°ê³¼: {len(ensemble_docs)}ê°œ")
        return ensemble_docs
    
    def _weighted_merge(
        self, 
        bm25_docs: List[Document], 
        vector_docs: List[Document]
    ) -> List[Document]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¬¸ì„œ í†µí•©"""
        doc_scores = {}
        
        # BM25 ì ìˆ˜ (40%)
        for i, doc in enumerate(bm25_docs):
            doc_id = doc.page_content[:100]  # ì‹ë³„ìš©
            score = self.bm25_weight * (1.0 / (i + 1))
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score
        
        # Vector ì ìˆ˜ (60%)
        for i, doc in enumerate(vector_docs):
            doc_id = doc.page_content[:100]
            score = self.vector_weight * (1.0 / (i + 1))
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        all_docs = {doc.page_content[:100]: doc for doc in bm25_docs + vector_docs}
        sorted_ids = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [all_docs[doc_id] for doc_id, _ in sorted_ids]
```

**íŠ¹ì§•**:
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 (40%) + Vector (60%)
- 2ë‹¨ê³„ í´ë°± ì´ˆê¸°í™” (ì•ˆì •ì„±)
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¬¸ì„œ í†µí•©

---

### 4.4 ReciprocalRankFusion (ìˆœìœ„ í†µí•©)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 315-370)

**ëª©ì **: ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©

**ì½”ë“œ**:
```python
class ReciprocalRankFusion:
    """ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Reciprocal Rank Fusionìœ¼ë¡œ í†µí•©"""
    
    def __init__(self, k: int = 60):
        """
        Args:
            k: RRF ìƒìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 60 ì‚¬ìš©)
        """
        self.k = k
    
    def fuse(
        self, 
        doc_lists: List[List[Document]], 
        top_k: int = 20
    ) -> List[Document]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ RRFë¡œ í†µí•©
        
        RRF ê³µì‹: score(d) = Î£ 1/(k + rank(d))
        """
        doc_scores = {}
        doc_objects = {}
        
        # ê° ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´
        for doc_list in doc_lists:
            for rank, doc in enumerate(doc_list, start=1):
                doc_id = doc.page_content[:100]  # ì‹ë³„ìš© ID
                
                # RRF ì ìˆ˜ ê³„ì‚°
                score = 1.0 / (self.k + rank)
                
                # ì ìˆ˜ ëˆ„ì 
                if doc_id in doc_scores:
                    doc_scores[doc_id] += score
                else:
                    doc_scores[doc_id] = score
                    doc_objects[doc_id] = doc
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_docs = sorted(
            doc_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        # Top K ë¬¸ì„œ ë°˜í™˜
        result = [doc_objects[doc_id] for doc_id, _ in sorted_docs[:top_k]]
        
        print(f"ğŸ”— RRF: {len(doc_lists)}ê°œ ë¦¬ìŠ¤íŠ¸ â†’ Top {len(result)}ê°œ")
        return result
```

**íŠ¹ì§•**:
- ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ í†µí•© (3ê°œ ì¿¼ë¦¬ Ã— 2ê°œ ë°©ì‹ = 6ê°œ ê²°ê³¼)
- RRF ì•Œê³ ë¦¬ì¦˜: `score = 1/(k + rank)`
- Top 20ê°œ ì„ ì •

---

### 4.5 ConversationMemory (ëŒ€í™” ê¸°ë¡)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 375-420)

**ëª©ì **: ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ

**ì½”ë“œ**:
```python
@dataclass
class ConversationMemory:
    """ëŒ€í™” ë§¥ë½ ê´€ë¦¬ (ìµœê·¼ Ní„´)"""
    messages: List[Dict[str, str]] = field(default_factory=list)
    max_turns: int = 3  # ìµœëŒ€ 3í„´ (6ê°œ ë©”ì‹œì§€)
    
    def add_user_message(self, content: str):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append({"role": "user", "content": content})
        self._trim_history()
    
    def add_assistant_message(self, content: str):
        """AI ì‘ë‹µ ì¶”ê°€"""
        self.messages.append({"role": "assistant", "content": content})
        self._trim_history()
    
    def _trim_history(self):
        """ìµœëŒ€ í„´ ìˆ˜ ìœ ì§€ (FIFO)"""
        max_messages = self.max_turns * 2  # ì‚¬ìš©ì + AI
        if len(self.messages) > max_messages:
            self.messages = self.messages[-max_messages:]
    
    def get_context(self) -> str:
        """ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        if not self.messages:
            return "ì—†ìŒ"
        
        context = []
        for msg in self.messages:
            role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
            context.append(f"{role}: {msg['content']}")
        
        return "\n".join(context)
    
    def clear(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.messages = []
```

**íŠ¹ì§•**:
- ìµœê·¼ 3í„´ (6ê°œ ë©”ì‹œì§€) ìœ ì§€
- FIFO ë°©ì‹ ìë™ ì •ë¦¬
- ë¬¸ìì—´ í¬ë§· ë³€í™˜ ê¸°ëŠ¥

---

### 4.6 AdvancedRAGPipeline (í†µí•© íŒŒì´í”„ë¼ì¸)

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 425-640)

**ëª©ì **: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

**í•µì‹¬ ë©”ì„œë“œ: `query()`**

**ì½”ë“œ**:
```python
def query(self, user_query: str) -> Dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (7ë‹¨ê³„)
    
    Returns:
        {
            "answer": "ì „ì²´ ë‹µë³€",
            "summary": "ìš”ì•½",
            "documents": [ê²€ìƒ‰ëœ ë¬¸ì„œë“¤],
            "metadata": {ì‹¤í–‰ ì •ë³´}
        }
    """
    print("\n" + "="*70)
    print(f"ğŸ“ ì§ˆë¬¸: {user_query}")
    print("="*70)
    
    start_time = datetime.now()
    
    # 1ï¸âƒ£ Router: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
    if self.router:
        route_result = self.router.route(user_query)
        if not route_result['is_valid']:
            return {
                "answer": f"âŒ {route_result['reason']}",
                "summary": "",
                "documents": [],
                "metadata": {"error": "invalid_query"}
            }
        query = route_result['refined_query']
    else:
        query = user_query
    
    # 2ï¸âƒ£ Multi-Query: 3ê°œ ì¿¼ë¦¬ ìƒì„±
    queries = [query]
    if self.multi_query:
        generated = self.multi_query.generate(query)
        queries = generated  # ìƒì„±ëœ 3ê°œë§Œ ì‚¬ìš©
    
    # 3ï¸âƒ£ Ensemble Retriever: ê° ì¿¼ë¦¬ë§ˆë‹¤ ê²€ìƒ‰
    all_doc_lists = []
    if self.ensemble:
        for q in queries:
            docs = self.ensemble.retrieve(q)
            all_doc_lists.append(docs)
    else:
        # í´ë°±: Vector ê²€ìƒ‰ë§Œ
        for q in queries:
            docs = self.vectorstore.similarity_search(q, k=20)
            all_doc_lists.append(docs)
    
    # 4ï¸âƒ£ RRF: ê²€ìƒ‰ ê²°ê³¼ í†µí•©
    if self.rrf:
        docs = self.rrf.fuse(all_doc_lists, top_k=20)
    else:
        # í´ë°±: ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš©
        docs = all_doc_lists[0] if all_doc_lists else []
    
    # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš© (í† í° ì œí•œ)
    docs = docs[:10]
    
    # 5ï¸âƒ£ Memory: ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    context = ""
    if self.memory:
        context = self.memory.get_context()
    
    # 6ï¸âƒ£ LLM: ìµœì¢… ë‹µë³€ ìƒì„±
    documents_text = "\n\n".join([
        f"[ì •ì±… {i+1}]\n{doc.page_content}\në©”íƒ€ë°ì´í„°: {doc.metadata}"
        for i, doc in enumerate(docs)
    ])
    
    response = self.answer_prompt | self.llm | StrOutputParser()
    answer = response.invoke({
        "query": user_query,
        "documents": documents_text,
        "context": context
    })
    
    # 7ï¸âƒ£ Summary: Chain of Thought ìš”ì•½ ìƒì„±
    summary_response = self.summary_prompt | self.llm | StrOutputParser()
    summary = summary_response.invoke({"answer": answer})
    
    # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
    if self.memory:
        self.memory.add_user_message(user_query)
        self.memory.add_assistant_message(answer)
    
    # ì‹¤í–‰ ì‹œê°„
    elapsed = (datetime.now() - start_time).total_seconds()
    
    print(f"\nâ±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print("="*70)
    
    return {
        "answer": answer,
        "summary": summary,
        "documents": docs,
        "metadata": {
            "query_count": len(queries),
            "document_count": len(docs),
            "elapsed_time": elapsed,
            "llm_calls": 4  # Router + MultiQuery + Answer + Summary
        }
    }
```

**íŠ¹ì§•**:
- 7ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
- ê° ë‹¨ê³„ë³„ ë¡œê¹…
- ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°±
- ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- ë©”íƒ€ë°ì´í„° ë°˜í™˜

---

## 5. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™

### 5.1 ChromaDB ì´ˆê¸°í™”

**íŒŒì¼**: `src/advanced_rag_pipeline.py` (Line 548-640)

**í•¨ìˆ˜**: `initialize_rag_pipeline()`

**ì½”ë“œ**:
```python
def initialize_rag_pipeline(vectordb_path=None, api_key=None):
    """
    Streamlit ì—°ë™ì„ ìœ„í•œ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    
    Args:
        vectordb_path: ë²¡í„° DB ê²½ë¡œ (Noneì´ë©´ ìë™ ê³„ì‚°)
        api_key: OpenAI API Key (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    
    Returns:
        AdvancedRAGPipeline: ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸
    """
    print("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    
    # 1. API Key ì„¤ì •
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
    elif not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # 2. LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,
        max_tokens=2048
    )
    
    # 3. Embeddings ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )
    
    # 4. VectorStore ë¡œë“œ
    if vectordb_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        vectordb_path = os.path.join(project_root, "data", "vectordb")
    
    if not os.path.exists(vectordb_path):
        raise FileNotFoundError(f"VectorDB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vectordb_path}")
    
    vectorstore = Chroma(
        persist_directory=vectordb_path,
        embedding_function=embeddings
    )
    
    # 5. ë¬¸ì„œ ë¡œë“œ (BM25ìš©)
    all_docs = vectorstore.get()
    documents = [
        Document(
            page_content=doc,
            metadata=meta
        )
        for doc, meta in zip(all_docs['documents'], all_docs['metadatas'])
    ]
    
    print(f"âœ… ë¬¸ì„œ ë¡œë“œ: {len(documents)}ê°œ")
    
    # 6. RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
    rag = AdvancedRAGPipeline(
        documents=documents,
        vectorstore=vectorstore,
        llm=llm,
        enable_router=True,
        enable_multi_query=True,
        enable_ensemble=True,
        enable_rrf=True,
        enable_memory=True,
        bm25_weight=0.4,
        vector_weight=0.6
    )
    
    print("âœ… RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    return rag
```

**íŠ¹ì§•**:
- í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ API Key ì„¤ì •
- ìë™ ê²½ë¡œ ê³„ì‚° (ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€)
- ë¬¸ì„œ ë¡œë“œ ë° ë³€í™˜ (BM25ìš©)
- ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í™œì„±í™”

---

### 5.2 ë²¡í„° DB êµ¬ì¶•

**íŒŒì¼**: `notebooks/build_vectordb.py` (Line 1-572)

**ì£¼ìš” í•¨ìˆ˜**:

#### 1) ë°ì´í„° ë¡œë“œ
```python
def load_preprocessed_data(filepath):
    """
    ì „ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ
    
    Returns:
        list: ì •ì±… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (3,550ê°œ)
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ")
    return data
```

#### 2) ì„ë² ë”© ìƒì„±
```python
def create_embeddings_batch(texts, model="text-embedding-3-small", batch_size=100):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± (API ì œí•œ íšŒí”¼)
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        model: ì„ë² ë”© ëª¨ë¸
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        list: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (1,536ì°¨ì›)
    """
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        try:
            response = client.embeddings.create(
                input=batch,
                model=model
            )
            
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
            
            print(f"  ë°°ì¹˜ {i//batch_size + 1}/{total_batches} ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì¬ì‹œë„
            time.sleep(5)
    
    return all_embeddings
```

#### 3) ChromaDB ì €ì¥
```python
def save_to_chromadb(policies, embeddings):
    """
    ChromaDBì— ì €ì¥
    
    Args:
        policies: ì •ì±… ë°ì´í„° (ë©”íƒ€ë°ì´í„°)
        embeddings: ë²¡í„° ì„ë² ë”©
    """
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(
        path="./data/vectordb",
        settings=Settings(anonymized_telemetry=False)
    )
    
    # ì»¬ë ‰ì…˜ ìƒì„± (ê¸°ì¡´ ì‚­ì œ)
    try:
        chroma_client.delete_collection(name="youth_policies")
    except:
        pass
    
    collection = chroma_client.create_collection(
        name="youth_policies",
        metadata={"description": "ì²­ë…„ ì •ì±… ë²¡í„° DB"}
    )
    
    # ë°°ì¹˜ ì €ì¥
    batch_size = 100
    for i in range(0, len(policies), batch_size):
        batch_policies = policies[i:i+batch_size]
        batch_embeddings = embeddings[i:i+batch_size]
        
        ids = [f"policy_{i+j}" for j in range(len(batch_policies))]
        documents = [p['ì •ì±…ì„¤ëª…'] for p in batch_policies]
        metadatas = [
            {
                "ì •ì±…ëª…": p.get('ì •ì±…ëª…', ''),
                "ì§€ì—­": p.get('ì§€ì—­', ''),
                "ì—°ë ¹": f"{p.get('ì§€ì›ìµœì†Œì—°ë ¹', '0')}-{p.get('ì§€ì›ìµœëŒ€ì—°ë ¹', '99')}",
                "ì •ì±…ìœ í˜•": p.get('ëŒ€ë¶„ë¥˜', '')
            }
            for p in batch_policies
        ]
        
        collection.add(
            ids=ids,
            documents=documents,
            embeddings=batch_embeddings,
            metadatas=metadatas
        )
        
        print(f"  ì €ì¥: {i+len(batch_policies)}/{len(policies)}")
    
    print("âœ… ChromaDB ì €ì¥ ì™„ë£Œ!")
```

**íŠ¹ì§•**:
- ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
- API ì œí•œ ëŒ€ì‘ (ì¬ì‹œë„ ë¡œì§)
- ë©”íƒ€ë°ì´í„° ë§¤í•‘
- ì§„í–‰ë¥  í‘œì‹œ

---

## 6. ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

### 6.1 Streamlit ì•± êµ¬ì¡°

**íŒŒì¼**: `src/streamlit_app.py`

**ì£¼ìš” ì»´í¬ë„ŒíŠ¸**:

#### 1) CSS ìŠ¤íƒ€ì¼
```python
def apply_custom_css():
    """ì»¤ìŠ¤í…€ CSS ì ìš©"""
    st.markdown("""
    <style>
    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    
    .summary-box {
        background-color: #fff9c4;
        border-left: 4px solid #fbc02d;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    
    .policy-card {
        background-color: #ffffff;
        border: 1px solid #e0e0e0;
        border-radius: 0.5rem;
        padding: 1rem;
        margin-bottom: 0.5rem;
    }
    </style>
    """, unsafe_allow_html=True)
```

#### 2) RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
```python
@st.cache_resource
def load_rag_pipeline():
    """
    RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ìºì‹±)
    
    @st.cache_resource: ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ ìœ ì§€
    """
    try:
        return initialize_rag_pipeline()
    except Exception as e:
        st.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
```

#### 3) ì§ˆë¬¸ ì¸í„°í˜ì´ìŠ¤
```python
def render_question_interface(rag):
    """ì§ˆë¬¸ ì…ë ¥ ë° ë‹µë³€ ì¸í„°í˜ì´ìŠ¤"""
    st.subheader("â“ ì²­ë…„ì •ì±… ì§ˆë¬¸í•˜ê¸°")
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.chat_history:
        role = message.get("role")
        
        with st.chat_message(role):
            if role == "assistant":
                # ìš”ì•½ í‘œì‹œ
                if "summary" in message:
                    st.markdown(
                        f'<div class="summary-box">'
                        f'<strong>ğŸ“Œ ìš”ì•½</strong><br>{message["summary"]}'
                        f'</div>',
                        unsafe_allow_html=True
                    )
                
                # ì „ì²´ ë‹µë³€ í‘œì‹œ
                st.markdown(message["content"])
                
                # ê²€ìƒ‰ëœ ì •ì±… í‘œì‹œ
                if "documents" in message and message["documents"]:
                    with st.expander(f"ğŸ“Š ê²€ìƒ‰ëœ ì •ì±… ({len(message['documents'])}ê°œ)"):
                        for i, doc in enumerate(message["documents"][:5], 1):
                            metadata = doc.metadata
                            st.markdown(f"""
                            <div class="policy-card">
                                <strong>{i}. {metadata.get('ì •ì±…ëª…', 'N/A')}</strong><br>
                                ğŸ“ {metadata.get('ì§€ì—­', 'N/A')}<br>
                                ğŸ¯ {metadata.get('ì •ì±…ìœ í˜•', 'N/A')}<br>
                                ğŸ‘¥ ì—°ë ¹: {metadata.get('ì—°ë ¹', 'N/A')}<br>
                            </div>
                            """, unsafe_allow_html=True)
            else:
                st.markdown(message["content"])
    
    # ì§ˆë¬¸ ì…ë ¥
    if question := st.chat_input("ì²­ë…„ ì •ì±…ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "user", "content": question})
        
        # RAG ì‘ë‹µ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = rag.query(question)
            
            # ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": result.get("answer", ""),
                "summary": result.get("summary", ""),
                "documents": result.get("documents", [])
            })
        
        st.rerun()  # í™”ë©´ ê°±ì‹ 
```

#### 4) ë©”ì¸ í•¨ìˆ˜
```python
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ì²­ë…„ì •ì±… Q&A ì±—ë´‡",
        page_icon="ğŸ“",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSS ì ìš©
    apply_custom_css()
    
    # íƒ€ì´í‹€
    st.markdown('<h1 class="main-title">ğŸ“ ì²­ë…„ ì •ì±… Q&A ì±—ë´‡</h1>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # RAG ë¡œë“œ ë²„íŠ¼
        if st.button("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ", type="primary"):
            rag = load_rag_pipeline()
            if rag:
                st.session_state["rag_pipeline"] = rag
                st.success("âœ… ë¡œë“œ ì™„ë£Œ!")
        
        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            if "rag_pipeline" in st.session_state:
                st.session_state.rag_pipeline.clear_memory()
            st.rerun()
        
        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        if "rag_pipeline" in st.session_state:
            st.success("ğŸŸ¢ RAG íŒŒì´í”„ë¼ì¸ í™œì„±í™”")
            st.info("""
            **í™œì„±í™”ëœ ê¸°ëŠ¥:**
            - ğŸ” MultiQuery (3ê°œ ì¿¼ë¦¬ ìƒì„±)
            - ğŸ“Š BM25 + Vector ê²€ìƒ‰
            - ğŸ’¬ ëŒ€í™” ê¸°ë¡ (ìµœê·¼ 3í„´)
            - ğŸ“Œ Chain of Thought ìš”ì•½
            """)
    
    # ë©”ì¸: ì§ˆë¬¸ ì¸í„°í˜ì´ìŠ¤
    if "rag_pipeline" in st.session_state:
        render_question_interface(st.session_state.rag_pipeline)
    else:
        st.info("ğŸ‘ˆ ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")
```

**íŠ¹ì§•**:
- ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ (`st.session_state`)
- ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
- ë°˜ì‘í˜• UI (ëŒ€í™” ê¸°ë¡ í‘œì‹œ)
- ì—ëŸ¬ í•¸ë“¤ë§

---

## 7. ì£¼ìš” ê¸°ëŠ¥ ì½”ë“œ

### 7.1 í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬

**íŒŒì¼**: `.env`

```bash
# OpenAI API
OPENAI_API_KEY=sk-...

# ì˜¨í†µì²­ë…„ API
YOUTH_POLICY_API=your_api_key
```

**ë¡œë“œ ë°©ë²•**:
```python
from dotenv import load_dotenv
import os

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

### 7.2 ì—ëŸ¬ í•¸ë“¤ë§

**BM25 ì´ˆê¸°í™” ì‹¤íŒ¨ ì²˜ë¦¬**:
```python
try:
    self.bm25_retriever = BM25Retriever.from_documents(...)
except TypeError:
    # í´ë°±: ì§ì ‘ ì´ˆê¸°í™”
    self.bm25_retriever = BM25Retriever(docs=...)
except Exception as e:
    print(f"âŒ BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    self.bm25_retriever = None
```

**JSON íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬**:
```python
try:
    result = json.loads(result_str)
except json.JSONDecodeError:
    # ê¸°ë³¸ê°’ ë°˜í™˜
    result = {
        "is_valid": True,
        "category": "ì¼ë°˜ì§ˆë¬¸",
        "refined_query": query
    }
```

### 7.3 ë¡œê¹… ë° ë””ë²„ê¹…

**ì§„í–‰ ìƒí™© ì¶œë ¥**:
```python
print(f"ğŸ”€ Router: {result['category']} | Valid: {result['is_valid']}")
print(f"ğŸ” Multi-Query Generated: {len(queries)}ê°œ")
print(f"  BM25: {len(bm25_docs)}ê°œ")
print(f"  Vector: {len(vector_docs)}ê°œ")
print(f"âœ… Ensemble ê²°ê³¼: {len(ensemble_docs)}ê°œ")
print(f"ğŸ”— RRF: {len(doc_lists)}ê°œ ë¦¬ìŠ¤íŠ¸ â†’ Top {len(result)}ê°œ")
print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
```

---

## 8. API ë° ì™¸ë¶€ ì—°ë™

### 8.1 OpenAI API ì—°ë™

**LLM (GPT-4o-mini)**:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,  # ì¼ê´€ì„± ë†’ì€ ë‹µë³€
    max_tokens=2048   # ìµœëŒ€ í† í°
)
```

**Embeddings (text-embedding-3-small)**:
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

### 8.2 ì˜¨í†µì²­ë…„ API ì—°ë™

**íŒŒì¼**: `notebooks/fetch_api_data.py`

**API í˜¸ì¶œ**:
```python
import requests

def fetch_youth_policies(page_size):
    api_url = "https://www.youthcenter.go.kr/go/ythip/getPlcy"
    params = {
        'apiKeyNm': YOUTH_POLICY_API,
        'pageSize': page_size
    }
    headers = {
        'User-Agent': 'Mozilla/5.0...',
        'Accept': 'application/json'
    }
    
    response = requests.get(api_url, params=params, headers=headers, timeout=60)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")
```

### 8.3 ChromaDB ì—°ë™

**ì €ì¥**:
```python
import chromadb

chroma_client = chromadb.PersistentClient(path="./data/vectordb")
collection = chroma_client.create_collection(name="youth_policies")

collection.add(
    ids=["policy_1", "policy_2", ...],
    documents=["ì •ì±… ì„¤ëª… 1", "ì •ì±… ì„¤ëª… 2", ...],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...], ...],
    metadatas=[{"ì •ì±…ëª…": "...", "ì§€ì—­": "..."}, ...]
)
```

**ë¡œë“œ**:
```python
from langchain_chroma import Chroma

vectorstore = Chroma(
    persist_directory="./data/vectordb",
    embedding_function=embeddings
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("ì„œìš¸ ì²­ë…„ ì£¼ê±° ì§€ì›", k=10)
```

---

## 9. ì‹¤í–‰ ë°©ë²•

### 9.1 í™˜ê²½ ì„¤ì •
```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„± (ì„ íƒ)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
# .env íŒŒì¼ ìƒì„± í›„ API Key ì…ë ¥
```

### 9.2 ë²¡í„° DB êµ¬ì¶• (ìµœì´ˆ 1íšŒ)
```bash
# 1. ë°ì´í„° ìˆ˜ì§‘
python notebooks/fetch_api_data.py

# 2. ë²¡í„° DB ìƒì„±
python notebooks/build_vectordb.py
```

### 9.3 Streamlit ì•± ì‹¤í–‰
```bash
streamlit run src/streamlit_app.py
```

### 9.4 Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
```python
from src.advanced_rag_pipeline import initialize_rag_pipeline

# RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
rag = initialize_rag_pipeline()

# ì§ˆë¬¸
result = rag.query("ì„œìš¸ì—ì„œ ì²­ë…„ ì£¼ê±° ì§€ì› ì •ì±… ì•Œë ¤ì¤˜")

# ê²°ê³¼ ì¶œë ¥
print("ë‹µë³€:", result["answer"])
print("ìš”ì•½:", result["summary"])
print("ë¬¸ì„œ ìˆ˜:", len(result["documents"]))
```

---

## 10. ì„±ëŠ¥ ë° ìµœì í™”

### 10.1 ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ |
|------|-----|
| **í‰ê·  ì‘ë‹µ ì‹œê°„** | 8-12ì´ˆ |
| **LLM í˜¸ì¶œ íšŸìˆ˜** | 4íšŒ (Router + MultiQuery + Answer + Summary) |
| **ê²€ìƒ‰ ì •í™•ë„** | ì•½ 85-90% (ìˆ˜ë™ í‰ê°€) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ì•½ 500 MB (ë²¡í„° DB ë¡œë“œ ì‹œ) |
| **ë™ì‹œ ì‚¬ìš©ì** | 10-20ëª… (Streamlit ê¸°ì¤€) |

### 10.2 ìµœì í™” ê¸°ë²•

1. **ìºì‹±**:
   - `@st.cache_resource`: RAG íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
   - LangChain ìºì‹±: ë™ì¼ ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±

2. **ë°°ì¹˜ ì²˜ë¦¬**:
   - ì„ë² ë”© ìƒì„± ì‹œ 100ê°œì”© ë°°ì¹˜
   - API í˜¸ì¶œ íšŸìˆ˜ ìµœì†Œí™”

3. **ê²€ìƒ‰ ìµœì í™”**:
   - Top 20ê°œë¡œ ì œí•œ (RRF)
   - ìµœì¢… 10ê°œë§Œ LLM ì…ë ¥

4. **ë©”ëª¨ë¦¬ ê´€ë¦¬**:
   - ëŒ€í™” ê¸°ë¡ 3í„´ìœ¼ë¡œ ì œí•œ
   - ë¶ˆí•„ìš”í•œ ë°ì´í„° ì œê±°

---

## 11. í–¥í›„ ê°œì„  ì‚¬í•­

### 11.1 ë‹¨ê¸° ê°œì„  (1-2ì£¼)
- [ ] ì‘ë‹µ ì†ë„ ìµœì í™” (6ì´ˆ ì´í•˜ ëª©í‘œ)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” (ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€)
- [ ] UI/UX ê°œì„  (ëª¨ë°”ì¼ ë°˜ì‘í˜•)

### 11.2 ì¤‘ê¸° ê°œì„  (1-2ê°œì›”)
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ì‹œìŠ¤í…œ (ì¢‹ì•„ìš”/ì‹«ì–´ìš”)
- [ ] ì •ì±… ì¶”ì²œ ê¸°ëŠ¥ (ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜)
- [ ] ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´, ì¤‘êµ­ì–´)

### 11.3 ì¥ê¸° ê°œì„  (3ê°œì›” ì´ìƒ)
- [ ] ìŒì„± ì¸í„°í˜ì´ìŠ¤ (STT/TTS)
- [ ] ì •ì±… ì•Œë¦¼ ì‹œìŠ¤í…œ (ì‹ ê·œ ì •ì±… í‘¸ì‹œ)
- [ ] ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ (í†µê³„, ë¡œê·¸)

---

## 12. ì°¸ê³  ìë£Œ

### 12.1 ì½”ë“œ ì €ì¥ì†Œ
- GitHub: `SKNETWORKS-FAMILY-AICAMP/SKN20-3rd-1TEAM`
- Branch: `main`

### 12.2 ê´€ë ¨ ë¬¸ì„œ
- [ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬](./DATA_COLLECTION_AND_PREPROCESSING.md)
- [í…ŒìŠ¤íŠ¸ ê³„íš ë° ê²°ê³¼](./TEST_REPORT.md)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](../README.md)

### 12.3 ì™¸ë¶€ ë¬¸ì„œ
- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/)
- [ChromaDB ê³µì‹ ë¬¸ì„œ](https://docs.trychroma.com/)
- [Streamlit ê³µì‹ ë¬¸ì„œ](https://docs.streamlit.io/)
- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs/)

---

**ì‘ì„±ì¼**: 2025-12-10  
**ì‘ì„±ì**: AI Assistant  
**ë²„ì „**: 1.0  
**ë¼ì´ì„ ìŠ¤**: MIT
