# ğŸ“¦ ê°œë°œëœ ì†Œí”„íŠ¸ì›¨ì–´: RAG ê¸°ë°˜ LLMê³¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ êµ¬í˜„ ì½”ë“œ

## 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### 4.1 QueryRouter (ì§ˆë¬¸ ê²€ì¦)

**íŒŒì¼**: `src/advanced_rag.py` (Line 52-100)

**ëª©ì **: ì‚¬ìš©ì ì§ˆë¬¸ì˜ ìœ íš¨ì„± ê²€ì¦ ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜

**ì½”ë“œ**:
```python
class QueryRouter:
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.router_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì •ì œí•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.

ì‘ì—…:
1. ì§ˆë¬¸ì´ ì˜ë¯¸ ìˆëŠ”ì§€ ê²€ì¦ (ì¸ì‚¬ë§, ìš•ì„¤, ë¬´ì˜ë¯¸í•œ ì…ë ¥ ì œì™¸)
2. ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì •ì±…ê²€ìƒ‰, ì¶”ì²œ, ì¼ë°˜ì§ˆë¬¸ ë“±)
3. LLMì´ ì²˜ë¦¬í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì •ì œ
4. ë§Œì•½ ì§ˆë¬¸ì— 'ì „êµ­', 'ì „ì²´', 'ëª¨ë“ ', 'ëª¨ë‘' ë“± ì „êµ­ ë‹¨ìœ„ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê³ , ì§€ì—­ëª…ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ refined_queryì—ì„œ 'ì „êµ­', 'ì „ì²´' ë“± ì§€ì—­ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ê³  í•µì‹¬ ì •ì±… í‚¤ì›Œë“œë§Œ ë‚¨ê²¨ì„œ ë” ì¼ë°˜í™”ëœ í˜•íƒœë¡œ ì •ì œí•˜ë¼. ì˜ˆë¥¼ ë“¤ì–´ 'ì „êµ­ ì¼ìë¦¬' â†’ 'ì¼ìë¦¬ ì •ì±…', 'ì „êµ­ ì²­ë…„ ë³µì§€' â†’ 'ì²­ë…„ ë³µì§€ ì •ì±…' ë“±ìœ¼ë¡œ ì •ì œ.
5. refined_queryëŠ” ë°˜ë“œì‹œ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë°˜í™˜í•˜ë¼.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "is_valid": true/false,
    "category": "ì •ì±…ê²€ìƒ‰|ì •ì±…ì¶”ì²œ|ì¼ë°˜ì§ˆë¬¸|ê¸°íƒ€",
    "refined_query": "ì •ì œëœ ì§ˆë¬¸",
    "reason": "íŒë‹¨ ì´ìœ "
}}

ì˜ˆì‹œ:
- ì…ë ¥: "ì „êµ­ ì¼ìë¦¬" â†’ refined_query: "ì¼ìë¦¬ ì •ì±…"
- ì…ë ¥: "ì „êµ­ ì²­ë…„ ë³µì§€" â†’ refined_query: "ì²­ë…„ ë³µì§€ ì •ì±…"
- ì…ë ¥: "ì„œìš¸ ì›”ì„¸ ì§€ì›" â†’ refined_query: "ì„œìš¸ ì›”ì„¸ ì§€ì› ì •ì±…"
- ì…ë ¥: "ì²­ë…„ ì •ì±…" â†’ refined_query: "ì²­ë…„ ì •ì±…"
"""),
            ("user", "{query}")
        ])
    
    def route(self, query: str) -> Dict:
        """ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ì •ì œ"""
        try:
            response = self.router_prompt | self.llm | StrOutputParser()
            result_str = response.invoke({"query": query})
            
            # JSON íŒŒì‹±
            result = json.loads(result_str)
            
            return result
        except Exception as e:
            return {
                "is_valid": True,
                "category": "ì¼ë°˜ì§ˆë¬¸",
                "refined_query": query,
                "reason": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ì›ë³¸ ì‚¬ìš©"
            }
```

**íŠ¹ì§•**:
- LLMì„ í™œìš©í•œ ì§€ëŠ¥í˜• ë¼ìš°íŒ…
- JSON í˜•ì‹ ì‘ë‹µìœ¼ë¡œ êµ¬ì¡°í™”ëœ ê²°ê³¼
- ì—ëŸ¬ í•¸ë“¤ë§ (íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©)

---

### 4.2 MultiQueryGenerator (ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±)

**íŒŒì¼**: `src/advanced_rag.py` (Line 105-160)

**ëª©ì **: í•˜ë‚˜ì˜ ì§ˆë¬¸ì„ 3ê°œì˜ ë‹¤ì–‘í•œ ê´€ì  ì¿¼ë¦¬ë¡œ í™•ì¥

**ì½”ë“œ**:
```python
class MultiQueryGenerator:
    """í•˜ë‚˜ì˜ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ í™•ì¥"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        
        self.multi_query_prompt = ChatPromptTemplate.from_messages([
            "system", """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ **ì˜ë„ì™€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìœ ì§€**í•œ ì±„ ê²€ìƒ‰ì— ìµœì í™”ëœ ì—¬ëŸ¬ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ í™•ì¥í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤ã€‚

**ì›ë³¸ ì§ˆë¬¸ì˜ ë‚´ìš©ì´ë‚˜ ì¡°ê±´ì„ ì„ì˜ë¡œ ì¶”ê°€í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ê²€ìƒ‰ ê´€ì ë§Œ ë‹¤ì–‘í™”í•´ì•¼ í•©ë‹ˆë‹¤ã€‚**

ì£¼ì–´ì§„ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”:

1.  **ì§€ì—­(Region) ì¶”ì¶œ ê°•ì œ: ì‚¬ìš©ìê°€ ì§€ì—­ì„ ì–¸ê¸‰í•˜ë©´, 'í•´ë‹¹ ì§€ì—­ + ì „êµ­' ì •ì±…ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤ã€‚ 
2.  **ì •ì±… í‚¤ì›Œë“œ(Policy Keyword): ì§ˆë¬¸ì˜ **í•µì‹¬ ì˜ë„**ì™€ ê´€ë ¨ëœ ì •ì±… í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ëœ ì •ì±…ë§Œ ë°˜í™˜í•  ê²ƒã€‚
3.  **ìœ ì‚¬í•œ ì˜ë¯¸ ë˜ëŠ” ê´€ë ¨ ì •ì±…ëª…**ì„ í¬í•¨í•˜ëŠ” ì¿¼ë¦¬ (ìœ ì˜ì–´ í™œìš©)

ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ ì—†ì´ ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”ã€‚""",
            ("user", "{query}")
        ])
    
    def generate(self, query: str) -> List[str]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±"""
        try:
            response = self.multi_query_prompt | self.llm | StrOutputParser()
            result = response.invoke({"query": query})
            
            # ì¿¼ë¦¬ ë¶„ë¦¬ (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
            queries = [q.strip() for q in result.split('\n') if q.strip()]
            # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
            all_queries = [query] + queries
            
            
            return all_queries
        
        except Exception as e:
            return [query]
```

**íŠ¹ì§•**:
- ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ (ë‹¨ì¼ ì¿¼ë¦¬ â†’ 3ê°œ ì¿¼ë¦¬)
- ë™ì˜ì–´ ë° ê´€ë ¨ ìš©ì–´ ìë™ ìƒì„±
- ì›ë³¸ ì¿¼ë¦¬ ì œì™¸ (ìƒì„±ëœ 3ê°œë§Œ ì‚¬ìš©)

---

### 4.3 EnsembleRetriever (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)

**íŒŒì¼**: `src/advanced_rag.py` (Line 165-310)

**ëª©ì **: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ + ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°í•©

**ì½”ë“œ**:
```python
class EnsembleRetriever:
    """Dense, BM25 ê²€ìƒ‰ì„ ê²°í•©í•œ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„"""
    
    def __init__(
        self, 
        documents: List[any],
        vectorstore: Chroma,
        llm: ChatOpenAI = None,
        bm25_k: int = 5,
        vector_k: int = 10,
        bm25_weight: float = 0.4,
        vector_weight: float = 0.6
    ):
        self.documents = documents
        self.vectorstore = vectorstore
        self.llm = llm
        
        # íŒŒë¼ë¯¸í„° ì €ì¥
        self.bm25_k = bm25_k
        self.vector_k = vector_k
        self.bm25_weight = bm25_weight
        self.vector_weight = vector_weight
        
        # ê° ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
        self._build_bm25()
        self._build_vector()
    
    def _build_bm25(self):
        """BM25 Retriever ìƒì„±"""
        if not RETRIEVERS_AVAILABLE or BM25Retriever is None:
            self.bm25_retriever = None
            return
        
        if not self.documents:
            self.bm25_retriever = None
            return
        
        try:
            # BM25Retriever ì´ˆê¸°í™” (from_documents ì‚¬ìš©)
            self.bm25_retriever = BM25Retriever.from_documents(
                documents=self.documents,
                k=self.bm25_k
            )
        except TypeError as e:
            # from_documentsê°€ ì‹¤íŒ¨í•˜ë©´ ì§ì ‘ ì´ˆê¸°í™” ì‹œë„
            try:
                self.bm25_retriever = BM25Retriever(docs=self.documents)
                self.bm25_retriever.k = self.bm25_k
            except Exception as e2:
                self.bm25_retriever = None
        except Exception as e:
            self.bm25_retriever = None
    
    def _build_vector(self):
        """Vector Retriever ìƒì„±"""
        try:
            # VectorStore ìƒíƒœ í™•ì¸
            test_search = self.vectorstore.similarity_search("í…ŒìŠ¤íŠ¸", k=1)
            
            self.vector_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.vector_k}
            )
        except Exception as e:
            self.vector_retriever = None
    
    def dense_search(self, query: str, metadata_filter: Dict = None) -> List[Tuple[any, float]]:
        """Dense ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)"""
        try:
            if self.vector_retriever:
                if metadata_filter:
                    # ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
                    docs = self.vectorstore.similarity_search(
                        query, 
                        k=self.vector_k,
                        filter=metadata_filter
                    )
                else:
                    docs = self.vector_retriever.invoke(query)
                
                results = [(doc, 1.0) for doc in docs]
                return results
            return []
        except Exception as e:
            return []
    
    def bm25_search(self, query: str) -> List[Tuple[any, float]]:
        """BM25 ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        try:
            if self.bm25_retriever:
                docs = self.bm25_retriever.invoke(query)
                results = [(doc, 1.0) for doc in docs]
                return results
            return []
        except Exception as e:
            return []
    
    def retrieve(self, queries: List[str], metadata_filter: Dict = None) -> Dict[str, List[Tuple[any, float]]]:
        """ëª¨ë“  ê²€ìƒ‰ ì „ëµ ì‹¤í–‰"""
        all_results = {
            'dense': [],
            'bm25': []
        }
        
        for query in queries:
            all_results['dense'].extend(self.dense_search(query, metadata_filter))
            all_results['bm25'].extend(self.bm25_search(query))
        
        return all_results
    
    def get_ensemble(self, query: str) -> List[any]:
        """Ensemble ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ì ìš©)"""
        if not RETRIEVERS_AVAILABLE or EnsembleRetriever is None:
            return self.dense_search(query)
        
        try:
            retrievers = []
            weights = []
            
            if self.bm25_retriever:
                retrievers.append(self.bm25_retriever)
                weights.append(self.bm25_weight)
            
            if self.vector_retriever:
                retrievers.append(self.vector_retriever)
                weights.append(self.vector_weight)
            
            if not retrievers:
                return []
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total_weight = sum(weights)
            weights = [w / total_weight for w in weights]
            
            # LangChainì˜ EnsembleRetriever ì‚¬ìš©
            ensemble = EnsembleRetriever(
                retrievers=retrievers,
                weights=weights
            )
            
            docs = ensemble.invoke(query)
            return docs
            
        except Exception as e:
            return []

```

**íŠ¹ì§•**:
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 (40%) + Vector (60%)
- 2ë‹¨ê³„ í´ë°± ì´ˆê¸°í™” (ì•ˆì •ì„±)
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¬¸ì„œ í†µí•©

---

### 4.4 ReciprocalRankFusion (ìˆœìœ„ í†µí•©)

**íŒŒì¼**: `src/advanced_rag.py` (Line 315-370)

**ëª©ì **: ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë­í‚¹ ê¸°ë°˜ìœ¼ë¡œ í†µí•©

**ì½”ë“œ**:
```python
class ReciprocalRankFusion:
    """ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë­í‚¹ ê¸°ë°˜ìœ¼ë¡œ í†µí•©"""
    def __init__(self, k: int = 60):
        self.k = k
    def fuse(self, results_dict: Dict[str, List[Tuple[any, float]]], top_k: int = 10) -> List[any]:
        doc_scores = {}
        for method, results in results_dict.items():
            for rank, (doc, score) in enumerate(results, 1):
                doc_id = doc.metadata.get('policy_id', id(doc))
                rrf_score = 1.0 / (self.k + rank)
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'score': 0}
                doc_scores[doc_id]['score'] += rrf_score
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        final_docs = [item[1]['doc'] for item in sorted_docs[:top_k]]
        return final_docs
```

**íŠ¹ì§•**:
- ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ í†µí•© (3ê°œ ì¿¼ë¦¬ Ã— 2ê°œ ë°©ì‹ = 6ê°œ ê²°ê³¼)
- RRF ì•Œê³ ë¦¬ì¦˜: `score = 1/(k + rank)`
- Top 20ê°œ ì„ ì •

---

### 4.5 ConversationMemory (ëŒ€í™” ê¸°ë¡)

**íŒŒì¼**: `src/advanced_rag.py` (Line 375-420)

**ëª©ì **: ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ

**ì½”ë“œ**:
```python
@dataclass
class ConversationMemory:
    messages: List[Dict] = field(default_factory=list)
    max_history: int = 10
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content, "timestamp": datetime.now().isoformat()})
        if len(self.messages) > self.max_history * 2:
            self.messages = self.messages[-self.max_history * 2:]
    def get_context(self) -> str:
        if not self.messages:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        context_parts = []
        for msg in self.messages[-6:]:
            role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "AI"
            context_parts.append(f"{role}: {msg['content']}")
        return "\n".join(context_parts)
    def clear(self):
        self.messages.clear()
```

**íŠ¹ì§•**:
- ìµœê·¼ 3í„´ (6ê°œ ë©”ì‹œì§€) ìœ ì§€
- FIFO ë°©ì‹ ìë™ ì •ë¦¬
- ë¬¸ìì—´ í¬ë§· ë³€í™˜ ê¸°ëŠ¥

---

### 4.6 AdvancedRAGPipeline (í†µí•© íŒŒì´í”„ë¼ì¸)

**íŒŒì¼**: `src/advanced_rag.py` (Line 425-640)

**ëª©ì **: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

**í•µì‹¬ ë©”ì„œë“œ: `query()`**

**ì½”ë“œ**:
```python
def query(self, user_query: str) -> Dict:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    # 1. Router: ì§ˆë¬¸ ê²€ì¦ ë° ì •ì œ
    if self.router:
        route_result = self.router.route(user_query)
        if not route_result['is_valid']:
            return {
                "answer": "ìŒâ€¦ ì´í•´ë¥¼ ëª» í–ˆì–´ğŸ˜¥. í•œ ë²ˆë§Œ ë‹¤ì‹œ ì–˜ê¸°í•´ì¤˜!",
                "documents": [],
                "metadata": route_result
            }
        query = route_result['refined_query']
    else:
        query = user_query
    
    # 2. Region Filter: ì§€ì—­ ì •ë³´ ì¶”ì¶œ ë° í•„í„° ìƒì„±
    metadata_filter = None
    region_info = None
    if self.region_filter:
        region_info = self.region_filter.detect_region(query)
        metadata_filter = self.region_filter.build_filter(region_info)
    
    # 3. Multi-Query: ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
    if self.multi_query:
        queries = self.multi_query.generate(query)
    else:
        queries = [query]
    
    # 4. Ensemble Retriever: ë‹¤ì¤‘ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©)
    if self.ensemble:
        search_results = self.ensemble.retrieve(queries, metadata_filter)
    else:
        if metadata_filter:
            docs_with_score = self.vectorstore.similarity_search_with_score(query, k=5, filter=metadata_filter)
        else:
            docs_with_score = self.vectorstore.similarity_search_with_score(query, k=5)
        search_results = {'dense': docs_with_score}
    
    # 5. RRF: ê²€ìƒ‰ ê²°ê³¼ í†µí•©
    if self.rrf:
        docs = self.rrf.fuse(search_results, top_k=20)
    else:
        docs = [doc for doc, score in search_results['dense']]
    
    # 6. Region Filter: ì§€ì—­ ê¸°ë°˜ í›„ì²˜ë¦¬ í•„í„°ë§
    if self.region_filter and region_info:
        docs = self.region_filter.filter_documents(docs, region_info)
    
    # 7. Memory: ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸°
    if self.memory:
        context = self.memory.get_context()
    else:
        context = "ì´ì „ ëŒ€í™” ì—†ìŒ"
    
    # 8. LLM: ìµœì¢… ë‹µë³€ ìƒì„± (ì •ì±…ëª… ì¤‘ë³µ ì—†ì´ ìµœëŒ€ 3ê°œë§Œ)
    seen_titles = set()
    unique_docs = []
    for doc in docs:
        title = doc.metadata.get('ì •ì±…ëª…', 'ì œëª© ì—†ìŒ')
        if title not in seen_titles:
            seen_titles.add(title)
            unique_docs.append(doc)
        if len(unique_docs) >= 3:
            break
    docs_text = "\\n\\n".join([
        f"[ì •ì±… {i+1}] {doc.metadata.get('ì •ì±…ëª…', 'ì œëª© ì—†ìŒ')}\\n{doc.page_content[:500]}"
        for i, doc in enumerate(unique_docs)
    ])
    try:
        response = self.answer_prompt | self.llm | StrOutputParser()
        answer = response.invoke({
            "context": context,
            "documents": docs_text,
            "query": user_query
        })
        summary_response = self.summary_prompt | self.llm | StrOutputParser()
        summary = summary_response.invoke({"answer": answer})
        if self.memory:
            self.memory.add_message("user", user_query)
            self.memory.add_message("assistant", answer)
        return {
            "answer": answer,
            "summary": summary,
            "documents": docs,
            "metadata": {
                "queries": queries,
                "num_docs_retrieved": len(docs),
                "has_context": bool(self.memory and self.memory.messages),
                "region_filter": metadata_filter
            }
        }
    except Exception as e:
        return {
            "answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "documents": [],
            "metadata": {"error": str(e)}
        }
```

**íŠ¹ì§•**:
- 7ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
- ê° ë‹¨ê³„ë³„ ë¡œê¹…
- ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°±
- ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- ë©”íƒ€ë°ì´í„° ë°˜í™˜

---

## 5. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™

### 5.1 ChromaDB ì´ˆê¸°í™”

**íŒŒì¼**: `src/advanced_rag.py` (Line 548-640)

**í•¨ìˆ˜**: `initialize_rag_pipeline()`

**ì½”ë“œ**:
```python
def initialize_rag_pipeline(vectordb_path: str = None, api_key: str = None):
    """
    Streamlitì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
    
    Args:
        vectordb_path: VectorDB ê²½ë¡œ (Noneì´ë©´ ìë™ ê³„ì‚°)
        api_key: OpenAI API Key (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    
    Returns:
        AdvancedRAGPipeline: ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸ ê°ì²´
    """
    # API Key ì„¤ì •
    if api_key:
        os.environ['OPENAI_API_KEY'] = api_key
    else:
        api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        raise ValueError('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    
    # LLM ë° ì„ë² ë”© ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.0,
        api_key=api_key
    )
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=api_key
    )
    
    # VectorDB ê²½ë¡œ ì„¤ì •
    if vectordb_path is None:
        vectordb_path = os.path.join(os.getcwd(), "data", "vectordb")
    
    if not os.path.exists(vectordb_path):
        raise FileNotFoundError(f"VectorDB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vectordb_path}")
    
    # VectorStore ë¡œë“œ
    vectorstore = Chroma(
        collection_name="youth_policies",
        embedding_function=embeddings,
        persist_directory=vectordb_path
    )
    
    # ë¬¸ì„œ ë¡œë“œ (BM25ë¥¼ ìœ„í•´ í•„ìš”)
    all_docs = vectorstore.get()
    
    if not all_docs or not all_docs.get('documents'):
        raise ValueError("VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    documents = []
    for i, doc_text in enumerate(all_docs['documents']):
        if doc_text and doc_text.strip():
            metadata = all_docs['metadatas'][i] if 'metadatas' in all_docs else {}
            documents.append(Document(page_content=doc_text, metadata=metadata))
    
    # RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
    rag = AdvancedRAGPipeline(
        documents=documents,
        vectorstore=vectorstore,
        llm=llm,
        enable_router=True,
        enable_multi_query=True,
        enable_ensemble=True,
        enable_rrf=True,
        enable_memory=True,
        enable_region_filter=True,
        bm25_k=5,
        vector_k=10,
        bm25_weight=0.4,
        vector_weight=0.6
    )
    
    return rag
```

**íŠ¹ì§•**:
- í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ API Key ì„¤ì •
- ìë™ ê²½ë¡œ ê³„ì‚° (ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€)
- ë¬¸ì„œ ë¡œë“œ ë° ë³€í™˜ (BM25ìš©)
- ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í™œì„±í™”

---

### 5.2 ë²¡í„° DB êµ¬ì¶•

**íŒŒì¼**: `notebooks/build_vectordb.py` (Line 1-572)

**ì£¼ìš” í•¨ìˆ˜**:

#### 1) ë°ì´í„° ë¡œë“œ
```python
def load_preprocessed_data(filepath):
    """
    ì „ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ
    
    Returns:
        list: ì •ì±… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (3,550ê°œ)
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ")
    return data
```

#### 2) ì„ë² ë”© ìƒì„±
```python
def create_embeddings_batch(texts, model="text-embedding-3-small", batch_size=100):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± (API ì œí•œ íšŒí”¼)
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        model: ì„ë² ë”© ëª¨ë¸
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        list: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (1,536ì°¨ì›)
    """
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        try:
            response = client.embeddings.create(
                input=batch,
                model=model
            )
            
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
            
            print(f"  ë°°ì¹˜ {i//batch_size + 1}/{total_batches} ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì¬ì‹œë„
            time.sleep(5)
    
    return all_embeddings
```

#### 3) ChromaDB ì €ì¥
```python
def save_to_chromadb(policies, embeddings):
    """
    ChromaDBì— ì €ì¥
    
    Args:
        policies: ì •ì±… ë°ì´í„° (ë©”íƒ€ë°ì´í„°)
        embeddings: ë²¡í„° ì„ë² ë”©
    """
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(
        path="./data/vectordb",
        settings=Settings(anonymized_telemetry=False)
    )
    
    # ì»¬ë ‰ì…˜ ìƒì„± (ê¸°ì¡´ ì‚­ì œ)
    try:
        chroma_client.delete_collection(name="youth_policies")
    except:
        pass
    
    collection = chroma_client.create_collection(
        name="youth_policies",
        metadata={"description": "ì²­ë…„ ì •ì±… ë²¡í„° DB"}
    )
    
    # ë°°ì¹˜ ì €ì¥
    batch_size = 100
    for i in range(0, len(policies), batch_size):
        batch_policies = policies[i:i+batch_size]
        batch_embeddings = embeddings[i:i+batch_size]
        
        ids = [f"policy_{i+j}" for j in range(len(batch_policies))]
        documents = [p['ì •ì±…ì„¤ëª…'] for p in batch_policies]
        metadatas = [
            {
                "ì •ì±…ëª…": p.get('ì •ì±…ëª…', ''),
                "ì§€ì—­": p.get('ì§€ì—­', ''),
                "ì—°ë ¹": f"{p.get('ì§€ì›ìµœì†Œì—°ë ¹', '0')}-{p.get('ì§€ì›ìµœëŒ€ì—°ë ¹', '99')}",
                "ì •ì±…ìœ í˜•": p.get('ëŒ€ë¶„ë¥˜', '')
            }
            for p in batch_policies
        ]
        
        collection.add(
            ids=ids,
            documents=documents,
            embeddings=batch_embeddings,
            metadatas=metadatas
        )
        
        print(f"  ì €ì¥: {i+len(batch_policies)}/{len(policies)}")
    
    print("âœ… ChromaDB ì €ì¥ ì™„ë£Œ!")
```

**íŠ¹ì§•**:
- ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
- API ì œí•œ ëŒ€ì‘ (ì¬ì‹œë„ ë¡œì§)
- ë©”íƒ€ë°ì´í„° ë§¤í•‘
- ì§„í–‰ë¥  í‘œì‹œ

---

## 6. ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

### 6.1 Streamlit ì•± êµ¬ì¡°

**íŒŒì¼**: `src/streamlit_app.py`

**ì£¼ìš” ì»´í¬ë„ŒíŠ¸**:

#### 1) CSS ìŠ¤íƒ€ì¼
```python
def apply_custom_css():
    """ì»¤ìŠ¤í…€ CSS ì ìš©"""
    st.markdown("""
    <style>
    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    
    .summary-box {
        background-color: #fff9c4;
        border-left: 4px solid #fbc02d;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    
    .policy-card {
        background-color: #ffffff;
        border: 1px solid #e0e0e0;
        border-radius: 0.5rem;
        padding: 1rem;
        margin-bottom: 0.5rem;
    }
    </style>
    """, unsafe_allow_html=True)
```

#### 2) RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
```python
@st.cache_resource
def load_rag_pipeline():
    """
    RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ìºì‹±)
    
    @st.cache_resource: ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ ìœ ì§€
    """
    try:
        return initialize_rag_pipeline()
    except Exception as e:
        st.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
```

#### 3) ì§ˆë¬¸ ì¸í„°í˜ì´ìŠ¤
```python
def render_question_interface(rag):
    """ì§ˆë¬¸ ì…ë ¥ ë° ë‹µë³€ ì¸í„°í˜ì´ìŠ¤"""
    st.subheader("â“ ì²­ë…„ì •ì±… ì§ˆë¬¸í•˜ê¸°")
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.chat_history:
        role = message.get("role")
        
        with st.chat_message(role):
            if role == "assistant":
                # ìš”ì•½ í‘œì‹œ
                if "summary" in message:
                    st.markdown(
                        f'<div class="summary-box">'
                        f'<strong>ğŸ“Œ ìš”ì•½</strong><br>{message["summary"]}'
                        f'</div>',
                        unsafe_allow_html=True
                    )
                
                # ì „ì²´ ë‹µë³€ í‘œì‹œ
                st.markdown(message["content"])
                
                # ê²€ìƒ‰ëœ ì •ì±… í‘œì‹œ
                if "documents" in message and message["documents"]:
                    with st.expander(f"ğŸ“Š ê²€ìƒ‰ëœ ì •ì±… ({len(message['documents'])}ê°œ)"):
                        for i, doc in enumerate(message["documents"][:5], 1):
                            metadata = doc.metadata
                            st.markdown(f"""
                            <div class="policy-card">
                                <strong>{i}. {metadata.get('ì •ì±…ëª…', 'N/A')}</strong><br>
                                ğŸ“ {metadata.get('ì§€ì—­', 'N/A')}<br>
                                ğŸ¯ {metadata.get('ì •ì±…ìœ í˜•', 'N/A')}<br>
                                ğŸ‘¥ ì—°ë ¹: {metadata.get('ì—°ë ¹', 'N/A')}<br>
                            </div>
                            """, unsafe_allow_html=True)
            else:
                st.markdown(message["content"])
    
    # ì§ˆë¬¸ ì…ë ¥
    if question := st.chat_input("ì²­ë…„ ì •ì±…ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "user", "content": question})
        
        # RAG ì‘ë‹µ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = rag.query(question)
            
            # ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": result.get("answer", ""),
                "summary": result.get("summary", ""),
                "documents": result.get("documents", [])
            })
        
        st.rerun()  # í™”ë©´ ê°±ì‹ 
```

#### 4) ë©”ì¸ í•¨ìˆ˜
```python
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ì²­ë…„ì •ì±… Q&A ì±—ë´‡",
        page_icon="ğŸ“",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # CSS ì ìš©
    apply_custom_css()
    
    # íƒ€ì´í‹€
    st.markdown('<h1 class="main-title">ğŸ“ ì²­ë…„ ì •ì±… Q&A ì±—ë´‡</h1>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # RAG ë¡œë“œ ë²„íŠ¼
        if st.button("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ", type="primary"):
            rag = load_rag_pipeline()
            if rag:
                st.session_state["rag_pipeline"] = rag
                st.success("âœ… ë¡œë“œ ì™„ë£Œ!")
        
        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            if "rag_pipeline" in st.session_state:
                st.session_state.rag_pipeline.clear_memory()
            st.rerun()
        
        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        if "rag_pipeline" in st.session_state:
            st.success("ğŸŸ¢ RAG íŒŒì´í”„ë¼ì¸ í™œì„±í™”")
            st.info("""
            **í™œì„±í™”ëœ ê¸°ëŠ¥:**
            - ğŸ” MultiQuery (3ê°œ ì¿¼ë¦¬ ìƒì„±)
            - ğŸ“Š BM25 + Vector ê²€ìƒ‰
            - ğŸ’¬ ëŒ€í™” ê¸°ë¡ (ìµœê·¼ 3í„´)
            - ğŸ“Œ Chain of Thought ìš”ì•½
            """)
    
    # ë©”ì¸: ì§ˆë¬¸ ì¸í„°í˜ì´ìŠ¤
    if "rag_pipeline" in st.session_state:
        render_question_interface(st.session_state.rag_pipeline)
    else:
        st.info("ğŸ‘ˆ ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")
```

**íŠ¹ì§•**:
- ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ (`st.session_state`)
- ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
- ë°˜ì‘í˜• UI (ëŒ€í™” ê¸°ë¡ í‘œì‹œ)
- ì—ëŸ¬ í•¸ë“¤ë§

---

## 7. ì£¼ìš” ê¸°ëŠ¥ ì½”ë“œ

### 7.1 í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬

**íŒŒì¼**: `.env`

```bash
# OpenAI API
OPENAI_API_KEY=sk-...

# ì˜¨í†µì²­ë…„ API
YOUTH_POLICY_API=your_api_key
```

**ë¡œë“œ ë°©ë²•**:
```python
from dotenv import load_dotenv
import os

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

### 7.2 ì—ëŸ¬ í•¸ë“¤ë§

**BM25 ì´ˆê¸°í™” ì‹¤íŒ¨ ì²˜ë¦¬**:
```python
try:
    self.bm25_retriever = BM25Retriever.from_documents(...)
except TypeError:
    # í´ë°±: ì§ì ‘ ì´ˆê¸°í™”
    self.bm25_retriever = BM25Retriever(docs=...)
except Exception as e:
    print(f"âŒ BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    self.bm25_retriever = None
```

**JSON íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬**:
```python
try:
    result = json.loads(result_str)
except json.JSONDecodeError:
    # ê¸°ë³¸ê°’ ë°˜í™˜
    result = {
        "is_valid": True,
        "category": "ì¼ë°˜ì§ˆë¬¸",
        "refined_query": query
    }
```

### 7.3 ë¡œê¹… ë° ë””ë²„ê¹…

**ì§„í–‰ ìƒí™© ì¶œë ¥**:
```python
print(f"ğŸ”€ Router: {result['category']} | Valid: {result['is_valid']}")
print(f"ğŸ” Multi-Query Generated: {len(queries)}ê°œ")
print(f"  BM25: {len(bm25_docs)}ê°œ")
print(f"  Vector: {len(vector_docs)}ê°œ")
print(f"âœ… Ensemble ê²°ê³¼: {len(ensemble_docs)}ê°œ")
print(f"ğŸ”— RRF: {len(doc_lists)}ê°œ ë¦¬ìŠ¤íŠ¸ â†’ Top {len(result)}ê°œ")
print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
```

---

## 8. API ë° ì™¸ë¶€ ì—°ë™

### 8.1 OpenAI API ì—°ë™

**LLM (GPT-4o-mini)**:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,  # ì¼ê´€ì„± ë†’ì€ ë‹µë³€
    max_tokens=2048   # ìµœëŒ€ í† í°
)
```

**Embeddings (text-embedding-3-small)**:
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

### 8.2 ì˜¨í†µì²­ë…„ API ì—°ë™

**íŒŒì¼**: `notebooks/fetch_api_data.py`

**API í˜¸ì¶œ**:
```python
import requests

def fetch_youth_policies(page_size):
    api_url = "https://www.youthcenter.go.kr/go/ythip/getPlcy"
    params = {
        'apiKeyNm': YOUTH_POLICY_API,
        'pageSize': page_size
    }
    headers = {
        'User-Agent': 'Mozilla/5.0...',
        'Accept': 'application/json'
    }
    
    response = requests.get(api_url, params=params, headers=headers, timeout=60)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")
```

### 8.3 ChromaDB ì—°ë™

**ì €ì¥**:
```python
import chromadb

chroma_client = chromadb.PersistentClient(path="./data/vectordb")
collection = chroma_client.create_collection(name="youth_policies")

collection.add(
    ids=["policy_1", "policy_2", ...],
    documents=["ì •ì±… ì„¤ëª… 1", "ì •ì±… ì„¤ëª… 2", ...],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...], ...],
    metadatas=[{"ì •ì±…ëª…": "...", "ì§€ì—­": "..."}, ...]
)
```

**ë¡œë“œ**:
```python
from langchain_chroma import Chroma

vectorstore = Chroma(
    persist_directory="./data/vectordb",
    embedding_function=embeddings
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("ì„œìš¸ ì²­ë…„ ì£¼ê±° ì§€ì›", k=10)
```
